{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15a0cb25-8dd6-4087-8559-9580abd2aa33",
   "metadata": {},
   "source": [
    "# Graph Neural Network for Simulated X-Ray Transient Detection\n",
    "The present work aims to train a GNN to label a particular sort of X-Ray transient using simulated events overlayed onto real data from XMM-Newton observations. We will experiment with Graph Convolutional Networks (GCNs). We will therefore  have to trandsform our point-cloud data into a \"k nearest neighbors\"-type graph. Data stored in the `raw` folder at the current working directory is taken from icaro.iusspavia.it `/mnt/data/PPS_ICARO_SIM2`. Observations store data for each photon detected, with no filter applied, in FITS files ending in `EVLI0000.FTZ` for the original observations and `EVLF0000.FTZ` for the observation and simulation combined. We will refer to the former data as \"genuine\" and to the latter as \"faked\" for brevity. \n",
    "\n",
    "IMPORTANT! Some of the \"faked\" data files downloaded are empty, so sun the `prune.py` script present in the current repository to remove these artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4feb140-ec55-4719-bb80-aa4202f0b39d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from astropy.table import Table, setdiff\n",
    "from astropy.table.operations import _join\n",
    "\n",
    "import torch\n",
    "import pyg_lib #new in torch_geometric 2.2.0!\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Dataset\n",
    "import torch_geometric.transforms as ttr\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "import torch_directml\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "from glob import glob\n",
    "from icecream import ic\n",
    "from tqdm import tqdm, trange\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dfe4373-7997-4845-ae1c-25613cc0a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208eafa1-0a88-4767-a17e-53aed8b52824",
   "metadata": {},
   "source": [
    "I define a `log` function for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4024c1a-0b06-4d23-9246-6954a39db998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(logfile, forcemode=None, **loggings):\n",
    "    if not forcemode is None:\n",
    "        assert forcemode in [\"w\", \"a\"], f\"Error: `forcemode` is '{forcemode}'. Must be either 'w' or 'a'\"\n",
    "    print(*(f\"{key}: {value}\" for key, value in loggings.items()), sep=\"\\n\\t\", file=sys.stderr)\n",
    "    mode = \"w+\"\n",
    "    if osp.exists(logfile) and forcemode is None:\n",
    "        usrinpt=\"\"\n",
    "        while not usrinpt in [\"O\",\"E\",\"C\"]:\n",
    "            usrinpt = input(f\"Do you want to overwrite [O] or extend [E] already existing log file {logfile}? (C to cancel) [O,E,C] \")\n",
    "        if usrinpt == \"C\":\n",
    "            return\n",
    "        elif usrinpt == \"E\":\n",
    "            mode = \"a\"\n",
    "    elif not forcemode is None:\n",
    "        mode = forcemode\n",
    "    with open(logfile, mode) as lf:\n",
    "        print(*(f\"{key}: {value}\" for key, value in loggings.items()), sep=\"\\n\\t\", file=lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f2af0a-fd9f-415b-8ee9-8cb5558f0ddf",
   "metadata": {},
   "source": [
    "The following function definition is a copy-paste of the original `setdiff` function from [astropy sourcecode](https://docs.astropy.org/en/stable/_modules/astropy/table/operations.html), modified to return the indices of elemnts prensent in `table1` but not in `table2`. This will be used to mark simulated data overlayed onto the real observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad5d438-8164-4c73-bc86-a8cfe03a20c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setdiff_idx(table1, table2, keys=None):\n",
    "    if keys is None:\n",
    "        keys = table1.colnames\n",
    "\n",
    "    # Check that all keys are in table1 and table2\n",
    "    for tbl, tbl_str in ((table1, 'table1'), (table2, 'table2')):\n",
    "        diff_keys = np.setdiff1d(keys, tbl.colnames)\n",
    "        if len(diff_keys) != 0:\n",
    "            raise ValueError(\"The {} columns are missing from {}, cannot take \"\n",
    "                             \"a set difference.\".format(diff_keys, tbl_str))\n",
    "\n",
    "    # Make a light internal copy of both tables\n",
    "    t1 = table1.copy(copy_data=False)\n",
    "    t1.meta = {}\n",
    "    t1.keep_columns(keys)\n",
    "    t1['__index1__'] = np.arange(len(table1))  # Keep track of rows indices\n",
    "\n",
    "    # Make a light internal copy to avoid touching table2\n",
    "    t2 = table2.copy(copy_data=False)\n",
    "    t2.meta = {}\n",
    "    t2.keep_columns(keys)\n",
    "    # Dummy column to recover rows after join\n",
    "    t2['__index2__'] = np.zeros(len(t2), dtype=np.uint8)  # dummy column\n",
    "\n",
    "    t12 = _join(t1, t2, join_type='left', keys=keys,\n",
    "                metadata_conflicts='silent')\n",
    "\n",
    "    # If t12 index2 is masked then that means some rows were in table1 but not table2.\n",
    "    if hasattr(t12['__index2__'], 'mask'):\n",
    "        # Define bool mask of table1 rows not in table2\n",
    "        diff = t12['__index2__'].mask\n",
    "        # Get the row indices of table1 for those rows\n",
    "        idx = t12['__index1__'][diff]\n",
    "    else:\n",
    "        idx = []\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ebd8d8-2055-485f-b0bb-c8bc1f2cebb6",
   "metadata": {},
   "source": [
    "TODO: check why the following does not prevent warnings from happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb92735d-a9b9-43ad-a685-92369fff7b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from astropy import units as u\n",
    "newunits = [u.def_unit(\"PIXELS\", u.pixel),\n",
    "            u.def_unit(\"CHAN\", u.chan),\n",
    "            u.def_unit(\"CHANNEL\", u.chan),\n",
    "            u.def_unit(\"0.05 arcsec\", 0.05*u.arcsec)\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb2708-056a-42c6-a2de-d5ea1a1f6796",
   "metadata": {},
   "source": [
    "Let's define a function that reads from a XMM observation FITS file and returns a table with the relevent event attributes and a flag `ISFAKE` which is `True` for simulated events and `False` for genuine events. The function takes two arguments: the path to the genuine file and the path to the faked file. A column with name `ISFAKE` will be added where `True` values will label simulated events. The function will return the faked observations table's `TIME`, `X`, `Y`, `PI`, `FLAG`, and `ISFAKE` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0a7ac1-6e7b-4926-9769-ea4b7df62565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_events(genuine, simulated):\n",
    "    with u.add_enabled_units(newunits):\n",
    "        I_dat = Table.read(genuine, hdu=1)\n",
    "        F_dat = Table.read(simulated, hdu=1)\n",
    "\n",
    "    # assert all(I_dat['X'].mask), f\"{genuine}\"\n",
    "\n",
    "    I_dat = I_dat[np.logical_not(I_dat['X'].mask)]\n",
    "    F_dat = F_dat[np.logical_not(F_dat['X'].mask)]\n",
    "    \n",
    "    D_dat_idx = setdiff_idx(F_dat, I_dat)\n",
    "    \n",
    "    dat = F_dat\n",
    "    dat[\"ISFAKE\"] = np.zeros(len(dat), dtype=bool)\n",
    "    dat[\"ISFAKE\"][D_dat_idx] = True\n",
    "    return dat[\"TIME\", \"X\", \"Y\", \"PI\", \"FLAG\", \"ISFAKE\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "314a5072",
   "metadata": {},
   "source": [
    "Define search patterns for event files, both genuine and faked, to be searched within the `raw` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a7ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "genuine_pattern = \"*EVLI0000.FTZ\"\n",
    "faked_pattern   = \"*EVLF0000.FTZ\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9ebef63-2fae-4341-93e1-1d2df25260f0",
   "metadata": {},
   "source": [
    "We will now set up `Data` and `Dataset` specialized classes for our observation data precessing and handling.\n",
    "\n",
    "First we will define `IcaroData` as a data type in which the `pos` attribute is overridden by a `@property`. This new `pos` gets and sets values from the last three features of each row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc0323f-4a89-4419-8753-17120531b0dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IcaroData(Data):    \n",
    "    @property\n",
    "    def pos(self):\n",
    "        return self.x[:, -3:]\n",
    "    \n",
    "    @pos.setter\n",
    "    def pos(self, replace):\n",
    "        assert replace.shape == self.pos.shape\n",
    "        self.x[:, -3:] = replace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ddf199-8275-4cde-80a9-dcc9b24e5ff6",
   "metadata": {},
   "source": [
    "The following dataset structure is quite standard. Notice how we use as feature (`x` attribute) values from the `PI`, `FLAG`, `TIME`, `X`, and `Y` columns, where the last three will be used as `pos` for the data. Notice then that this `pos` is then transformed through the use of a `Standard Scaler` and saved into the `processed` folder in the current working directory. As for the target (`y` attribute) we will use the `ISFAKE` column. Notice that, since we need to transform bools into numerical values for computation on CUDA, simulated data is now labeled with `1`, while genuine data with `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b99d2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.multiprocessing import Manager\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "329b879f-bcc3-4711-bc8b-82c63a954ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IcaroDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "        self.device = device\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return list(sorted(list(glob(osp.join(self.raw_dir, genuine_pattern))) +\n",
    "                           list(glob(osp.join(self.raw_dir, faked_pattern)))))\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return list(map(lambda name: osp.join(self.processed_dir, osp.basename(name)+\".pt\"), \n",
    "                        glob(osp.join(self.raw_dir, faked_pattern))))\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 2\n",
    "\n",
    "    def _hidden_process(self, raw_path):\n",
    "        # Read data from `raw_path`.\n",
    "        raw_path = raw_path.split('|', 2)\n",
    "        # try:\n",
    "        #     dat = read_events(*raw_path)\n",
    "        # except Exception as e:\n",
    "        #     print(\"error at \", *raw_path)\n",
    "        #     raise e\n",
    "\n",
    "        dat = read_events(*raw_path)\n",
    "        \n",
    "        data = IcaroData(x  =torch.from_numpy(np.array([dat[\"PI\"], dat[\"FLAG\"], dat[\"TIME\"], dat[\"X\"], dat[\"Y\"]]).T).float(), \n",
    "                            y  =torch.from_numpy(np.array(dat[\"ISFAKE\"])).long())\n",
    "        \n",
    "        ss2 = StandardScaler()\n",
    "        ss2.fit(data.pos)\n",
    "        new_pos = ss2.transform(data.pos)\n",
    "        data.pos = torch.tensor(new_pos)\n",
    "        data.to(self.device)\n",
    "\n",
    "        if self.pre_filter is not None and not self.pre_filter(data):\n",
    "            return\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        torch.save(data, osp.join(self.processed_dir, osp.basename(raw_path[-1])+\".pt\"))\n",
    "        # del data\n",
    "\n",
    "    def process(self):\n",
    "        fnames = list(zip(sorted(glob(osp.join(self.raw_dir, genuine_pattern))), \n",
    "                          sorted(glob(osp.join(self.raw_dir, faked_pattern))))\n",
    "                     )\n",
    "        already_processed = list(map(lambda name: osp.basename(name), glob(osp.join(self.processed_dir, \"*\"))))\n",
    "        fnames = np.array([gname+'|'+fname for gname, fname in fnames if not osp.basename(fname)+\".pt\" in already_processed])\n",
    "        # manager = Manager()\n",
    "        # with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        #     executor.map(self._hidden_process, tqdm(fnames))\n",
    "\n",
    "        # manager = Manager()\n",
    "        # with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        #     executor.map(self._hidden_process, tqdm(fnames))\n",
    "\n",
    "        # hidden_process = np.vectorize(self._hidden_process)\n",
    "        # hidden_process(fnames)\n",
    "\n",
    "        for fname in tqdm(fnames):\n",
    "            self._hidden_process(fname)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, self.processed_file_names[idx]))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3b312-2746-4693-b1a0-392ade116736",
   "metadata": {},
   "source": [
    "Let's load data from the current working directory and pre-transform it into a `KNNGraph`. This might take some time the first load, as the processed files are built, but subsequent runs will be speedy.\n",
    "\n",
    "TODO: lots of warnings from astropy units when first processing. Gotta see what we can do about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8de251a-d32c-4d72-867c-21ed7595eac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "  0%|          | 0/4885 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading\n",
      "preprocessing\n",
      "preprocessing done\n",
      "data transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4885 [02:04<168:37:58, 124.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data saved\n",
      "reading\n",
      "preprocessing\n",
      "preprocessing done\n",
      "data transformed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/4885 [02:50<106:03:06, 78.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data saved\n",
      "reading\n",
      "preprocessing\n",
      "preprocessing done\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings(): # to avoid useless astropy units warnings (gotta check how to solve it)\n",
    "    warnings.simplefilter('ignore')\n",
    "    ds = IcaroDataset(os.getcwd(), pre_transform = ttr.KNNGraph(k=20), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc062311-e60b-4056-964f-0454d689b5f0",
   "metadata": {},
   "source": [
    "We now define a `Net` model, with parametrable number of GCN layers, in channels, hidden channels, and out channels. Each layer but the last has a user-given activation function (`relu` is the default) and a `softmax` output activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96fee35-7ea9-4aee-8f0c-d2f1f0accb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import MLP, GINConv, global_add_pool\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, activation_function=F.relu):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "            in_channels = hidden_channels\n",
    "\n",
    "        self.last_conv = GCNConv(hidden_channels, out_channels)\n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation_function(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.last_conv(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bbe1d-23bf-4ec3-bc9c-f2f91c501a79",
   "metadata": {},
   "source": [
    "Let us now set up the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdb1f1-bac5-401a-8e79-9941582d9fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset    = ds\n",
    "lr         = 0.01\n",
    "device = torch_directml.device()\n",
    "# device     = torch.device('cpu') # cries in low GPU memory space YoY\n",
    "# device     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 1\n",
    "epochs     = 100\n",
    "hidden_channels = 5\n",
    "num_layers = 2\n",
    "activation_function = F.relu #torch.sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65a420-1fe6-4258-b5ee-291230458228",
   "metadata": {},
   "source": [
    "Then we'll split and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb40cdb-5bbc-45da-af42-939a3d6342d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/xaco/tesi/Transient-Data-Analysis/processed/P0745170301M1S001MIEVLF0000.FTZ.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m      2\u001b[0m \u001b[39m#print(torch.cuda.memory_summary(device=None, abbreviated=False))\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m Net(dataset\u001b[39m.\u001b[39;49mnum_node_features, hidden_channels, dataset\u001b[39m.\u001b[39mnum_classes, num_layers, activation_function)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[39m#print(torch.cuda.memory_summary(device=None, abbreviated=False))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m train_dataset \u001b[39m=\u001b[39m dataset[\u001b[39mlen\u001b[39m(dataset) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m5\u001b[39m:]\n",
      "File \u001b[0;32m~/programs/miniconda3/envs/tesi/lib/python3.9/site-packages/torch_geometric/data/dataset.py:110\u001b[0m, in \u001b[0;36mDataset.num_node_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnum_node_features\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    109\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns the number of features per node in the dataset.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m    111\u001b[0m     \u001b[39m# Do not fill cache for `InMemoryDataset`:\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_data_list\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_list \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/programs/miniconda3/envs/tesi/lib/python3.9/site-packages/torch_geometric/data/dataset.py:239\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39mpresent).\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39mIn case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a list, a\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mtuple, or a :obj:`torch.Tensor` or :obj:`np.ndarray` of type long or\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39mbool, will return a subset of the dataset at the specified indices.\"\"\"\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, (\u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39minteger))\n\u001b[1;32m    236\u001b[0m         \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, Tensor) \u001b[39mand\u001b[39;00m idx\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m    237\u001b[0m         \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, np\u001b[39m.\u001b[39mndarray) \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misscalar(idx))):\n\u001b[0;32m--> 239\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices()[idx])\n\u001b[1;32m    240\u001b[0m     data \u001b[39m=\u001b[39m data \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(data)\n\u001b[1;32m    241\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n",
      "Cell \u001b[0;32mIn[10], line 70\u001b[0m, in \u001b[0;36mIcaroDataset.get\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m---> 70\u001b[0m     data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(osp\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessed_dir, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessed_file_names[idx]))\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/programs/miniconda3/envs/tesi/lib/python3.9/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/programs/miniconda3/envs/tesi/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/programs/miniconda3/envs/tesi/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/xaco/tesi/Transient-Data-Analysis/processed/P0745170301M1S001MIEVLF0000.FTZ.pt'"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "#print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "model = Net(dataset.num_node_features, hidden_channels, dataset.num_classes, num_layers, activation_function).to(device)\n",
    "#print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "train_dataset = dataset[len(dataset) // 5:]\n",
    "train_loader  = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "valid_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "valid_loader  = DataLoader(valid_dataset, batch_size+1)\n",
    "test_dataset  = dataset[:len(dataset) // 10]\n",
    "test_loader   = DataLoader(test_dataset, batch_size+1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849284eb-ee04-4f9c-9b57-c6c3b9359588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35698648-7837-4f42-aec6-393b86c49d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def total_len(dataset):\n",
    "    \"\"\"Returns the number of target rows of the dataset\"\"\"\n",
    "    return np.sum([len(data.y) for data in dataset])\n",
    "\n",
    "def total_positives(dataset):\n",
    "    \"\"\"Returns the number of target value '1' of the dataset (only if the other class is '0')\"\"\"\n",
    "    return np.sum([data.y.sum().item() for data in dataset])\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        loader = train_loader\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data).to(device)\n",
    "        pred = out.argmax(dim=-1)\n",
    "        totpos = total_positives(loader.dataset)\n",
    "        totlen = total_len(loader.dataset)\n",
    "        true_positives = torch.logical_and(pred == 1, pred == data.y).sum().int()/totpos\n",
    "        true_negatives = torch.logical_and(pred == 0, pred == data.y).sum().int()/(totlen-totpos)\n",
    "        frac, rev_frac = data.y.sum().item()/len(data.y), (len(data.y) - data.y.sum().item())/len(data.y)\n",
    "        assert not np.isnan(frac) and not np.isnan(rev_frac)\n",
    "        if frac == 0: # in this case placeholder parameters must be enforced to avoid unwanted behavior\n",
    "            frac = rev_frac = 0.5\n",
    "            true_positives = 1.\n",
    "        addloss = (true_positives*true_negatives)**(-0.5) - 1 # scares the model out of giving a constant answer\n",
    "        loss = F.cross_entropy(out, data.y, weight=torch.tensor([frac, rev_frac]).to(device)) + addloss\n",
    "        assert not torch.isnan(loss.detach()), f\"out: {out}\\ndata.y: {data.y}\\nLoss: {total_loss}\\nWeight: {frac}\"\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        #print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "        del data\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect\n",
    "        #print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "    return total_loss / total_len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_correct         = 0\n",
    "    total_true_positives  = 0\n",
    "    total_false_positives = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data).argmax(dim=-1)\n",
    "        total_correct += int((pred == data.y).sum())\n",
    "        total_true_positives += int(np.logical_and(pred == 1, pred == data.y).sum())\n",
    "        total_false_positives += int(np.logical_and(pred == 1, pred != data.y).sum())\n",
    "        del data\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect\n",
    "    totlen = total_len(loader.dataset)\n",
    "    totpos = total_positives(loader.dataset)\n",
    "    return (total_correct/totlen, \n",
    "            total_true_positives/totpos, \n",
    "            total_false_positives/(totlen-totpos)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe9a4a8-c112-4938-998c-6170c320b44c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train()\n",
    "    train_acc, train_tp, train_fp = test(train_loader)\n",
    "    test_acc, test_tp, test_fp = test(valid_loader)\n",
    "    log(Epoch=epoch, \n",
    "        AbsLogLoss=np.log(loss), \n",
    "        Train_accuracy=train_acc,\n",
    "        Train_true_positives=train_tp,\n",
    "        Train_false_positives=train_fp,\n",
    "        Test_accuracy=test_acc,\n",
    "        Test_true_positives=test_tp,\n",
    "        Test_false_positives=test_fp,\n",
    "        logfile=\"logs.log\",\n",
    "        forcemode=\"w\"\n",
    "       )\n",
    "    # if not epoch % 10: # for acoustic feedback\n",
    "    #     sp.run([\"spd-say\", \"'Epoch! Epoch! Epoch!'\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32629856-483f-4949-9587-4a184e1fe8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True: # warns that the process is finished\n",
    "#     sp.run([\"spd-say\", \"'Your process is done'\"])\n",
    "#     time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205ede8-ea00-4b08-b98c-7e7ba57b374c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tesi)",
   "language": "python",
   "name": "tesi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
