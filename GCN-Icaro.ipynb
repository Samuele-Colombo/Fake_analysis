{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a0cb25-8dd6-4087-8559-9580abd2aa33",
   "metadata": {},
   "source": [
    "# Graph Neural Network for Simulated X-Ray Transient Detection\n",
    "The present work aims to train a GNN to label a particular sort of X-Ray transient using simulated events overlayed onto real data from XMM-Newton observations. We will experiment with Graph Convolutional Networks (GCNs). We will therefore  have to trandsform our point-cloud data into a \"k nearest neighbors\"-type graph. Data stored in the `raw` folder at the current working directory is taken from icaro.iusspavia.it `/mnt/data/PPS_ICARO_SIM2`. Observations store data for each photon detected, with no filter applied, in FITS files ending in `EVLI0000.FTZ` for the original observations and `EVLF0000.FTZ` for the observation and simulation combined. We will refer to the former data as \"genuine\" and to the latter as \"faked\" for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4feb140-ec55-4719-bb80-aa4202f0b39d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from astropy.table import Table, setdiff\n",
    "from astropy.table.operations import _join\n",
    "\n",
    "import torch\n",
    "import pyg_lib #new in torch_geometric 2.2.0!\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Dataset\n",
    "import torch_geometric.transforms as ttr\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "from glob import glob\n",
    "from icecream import ic\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dfe4373-7997-4845-ae1c-25613cc0a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208eafa1-0a88-4767-a17e-53aed8b52824",
   "metadata": {},
   "source": [
    "I define a `log` function for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4024c1a-0b06-4d23-9246-6954a39db998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(logfile, forcemode=None, **loggings):\n",
    "    if not forcemode is None:\n",
    "        assert forcemode in [\"w\", \"a\"], f\"Error: `forcemode` is '{forcemode}'. Must be either 'w' or 'a'\"\n",
    "    print(*(f\"{key}: {value}\" for key, value in loggings.items()), sep=\"\\n\\t\", file=sys.stderr)\n",
    "    mode = \"w+\"\n",
    "    if osp.exists(logfile) and forcemode is None:\n",
    "        usrinpt=\"\"\n",
    "        while not usrinpt in [\"O\",\"E\",\"C\"]:\n",
    "            usrinpt = input(f\"Do you want to overwrite [O] or extend [E] already existing log file {logfile}? (C to cancel) [O,E,C] \")\n",
    "        if usrinpt == \"C\":\n",
    "            return\n",
    "        elif usrinpt == \"E\":\n",
    "            mode = \"a\"\n",
    "    elif not forcemode is None:\n",
    "        mode = forcemode\n",
    "    with open(logfile, mode) as lf:\n",
    "        print(*(f\"{key}: {value}\" for key, value in loggings.items()), sep=\"\\n\\t\", file=lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f2af0a-fd9f-415b-8ee9-8cb5558f0ddf",
   "metadata": {},
   "source": [
    "The following function definition is a copy-paste of the original `setdiff` function from [astropy sourcecode](https://docs.astropy.org/en/stable/_modules/astropy/table/operations.html), modified to return the indices of elemnts prensent in `table1` but not in `table2`. This will be used to mark simulated data overlayed onto the real observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad5d438-8164-4c73-bc86-a8cfe03a20c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setdiff_idx(table1, table2, keys=None):\n",
    "    if keys is None:\n",
    "        keys = table1.colnames\n",
    "\n",
    "    # Check that all keys are in table1 and table2\n",
    "    for tbl, tbl_str in ((table1, 'table1'), (table2, 'table2')):\n",
    "        diff_keys = np.setdiff1d(keys, tbl.colnames)\n",
    "        if len(diff_keys) != 0:\n",
    "            raise ValueError(\"The {} columns are missing from {}, cannot take \"\n",
    "                             \"a set difference.\".format(diff_keys, tbl_str))\n",
    "\n",
    "    # Make a light internal copy of both tables\n",
    "    t1 = table1.copy(copy_data=False)\n",
    "    t1.meta = {}\n",
    "    t1.keep_columns(keys)\n",
    "    t1['__index1__'] = np.arange(len(table1))  # Keep track of rows indices\n",
    "\n",
    "    # Make a light internal copy to avoid touching table2\n",
    "    t2 = table2.copy(copy_data=False)\n",
    "    t2.meta = {}\n",
    "    t2.keep_columns(keys)\n",
    "    # Dummy column to recover rows after join\n",
    "    t2['__index2__'] = np.zeros(len(t2), dtype=np.uint8)  # dummy column\n",
    "\n",
    "    t12 = _join(t1, t2, join_type='left', keys=keys,\n",
    "                metadata_conflicts='silent')\n",
    "\n",
    "    # If t12 index2 is masked then that means some rows were in table1 but not table2.\n",
    "    if hasattr(t12['__index2__'], 'mask'):\n",
    "        # Define bool mask of table1 rows not in table2\n",
    "        diff = t12['__index2__'].mask\n",
    "        # Get the row indices of table1 for those rows\n",
    "        idx = t12['__index1__'][diff]\n",
    "    else:\n",
    "        idx = []\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb2708-056a-42c6-a2de-d5ea1a1f6796",
   "metadata": {},
   "source": [
    "Let's define a function that reads from a XMM observation FITS file and returns a table with the relevent event attributes and a flag `ISFAKE` which is `True` for simulated events and `False` for genuine events. The function takes two arguments: the path to the genuine file and the path to the faked file. A column with name `ISFAKE` will be added where `True` values will label simulated events. The function will return the faked observations table's `TIME`, `X`, `Y`, `PI`, `FLAG`, and `ISFAKE` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f0a7ac1-6e7b-4926-9769-ea4b7df62565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_events(genuine, simulated):\n",
    "    I_dat = Table.read(genuine, hdu=1)\n",
    "    F_dat = Table.read(simulated, hdu=1)\n",
    "    \n",
    "    D_dat_idx = setdiff_idx(F_dat, I_dat)\n",
    "    \n",
    "    dat = F_dat\n",
    "    dat[\"ISFAKE\"] = np.zeros(len(dat), dtype=bool)\n",
    "    dat[\"ISFAKE\"][D_dat_idx] = True\n",
    "    return dat[\"TIME\", \"X\", \"Y\", \"PI\", \"FLAG\", \"ISFAKE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ebd8d8-2055-485f-b0bb-c8bc1f2cebb6",
   "metadata": {},
   "source": [
    "TODO: check why the following does not prevent warnings from happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb92735d-a9b9-43ad-a685-92369fff7b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from astropy import units as u\n",
    "newunits = [u.def_unit(\"PIXELS\", u.pixel),\n",
    "            u.def_unit(\"CHAN\", u.chan),\n",
    "            u.def_unit(\"CHANNEL\", u.chan),\n",
    "            u.def_unit(\"0.05 arcsec\", 0.05*u.arcsec)\n",
    "           ]\n",
    "u.add_enabled_units(newunits);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebef63-2fae-4341-93e1-1d2df25260f0",
   "metadata": {},
   "source": [
    "We will now set up `Data` and `Dataset` specialized classes fou our observation data precessing and handling.\n",
    "\n",
    "First we will define `IcaroData` as a data type in which the `pos` attribute is overridden by a `@property`. This new `pos` gets and sets values from the last three features of each row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfc0323f-4a89-4419-8753-17120531b0dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IcaroData(Data):    \n",
    "    @property\n",
    "    def pos(self):\n",
    "        return self.x[:, -3:]\n",
    "    \n",
    "    @pos.setter\n",
    "    def pos(self, replace):\n",
    "        assert replace.shape == self.pos.shape\n",
    "        self.x[:, -3:] = replace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ddf199-8275-4cde-80a9-dcc9b24e5ff6",
   "metadata": {},
   "source": [
    "The following dataset structure is quite standard. Notice how we use as feature (`x` attribute) values from the `PI`, `FLAG`, `TIME`, `X`, and `Y` columns, where the last three will be used as `pos` for the data. Notice then that this `pos` is then transformed through the use of a `Standard Scaler` and saved into the `processed` folder in the current working directory. As for the target (`y` attribute) we will use the `ISFAKE` column. Notice that, since we need to transform bools into numerical values for computation on CUDA, simulated data is now labeled with `1`, while genuine data with `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329b879f-bcc3-4711-bc8b-82c63a954ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IcaroDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return list(sorted(list(glob(osp.join(self.raw_dir, \"0*/pps/*EVLI0000.FTZ\"))) +\n",
    "                           list(glob(osp.join(self.raw_dir, \"0*/pps/*EVLF0000.FTZ\")))))\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return list(map(lambda name: osp.join(self.processed_dir, osp.basename(name)+\".pt\"), \n",
    "                        glob(osp.join(self.raw_dir, \"0*/pps/*EVLF0000.FTZ\"))))\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 2\n",
    "\n",
    "    def process(self):\n",
    "        fnames = list(zip(sorted(glob(osp.join(self.raw_dir, \"0*/pps/*EVLI0000.FTZ\"))), \n",
    "                          sorted(glob(osp.join(self.raw_dir, \"0*/pps/*EVLF0000.FTZ\"))))\n",
    "                     )\n",
    "        for raw_path in fnames:\n",
    "            # Read data from `raw_path`.\n",
    "            dat = read_events(*raw_path)\n",
    "            data = IcaroData(x  =torch.from_numpy(np.array([dat[\"PI\"], dat[\"FLAG\"], dat[\"TIME\"], dat[\"X\"], dat[\"Y\"]]).T).float(), \n",
    "                             y  =torch.from_numpy(np.array(dat[\"ISFAKE\"])).long())\n",
    "            \n",
    "            ss2 = StandardScaler()\n",
    "            ss2.fit(data.pos)\n",
    "            new_pos = ss2.transform(data.pos)\n",
    "            data.pos = torch.tensor(new_pos)\n",
    "\n",
    "            if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                continue\n",
    "\n",
    "            if self.pre_transform is not None:\n",
    "                data = self.pre_transform(data)\n",
    "\n",
    "            torch.save(data, osp.join(self.processed_dir, osp.basename(raw_path[-1])+\".pt\"))\n",
    "            #print(osp.basename(raw_path[-1])+\".pt\")\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, self.processed_file_names[idx]))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3b312-2746-4693-b1a0-392ade116736",
   "metadata": {},
   "source": [
    "Let's load data from the current working directory and pre-transform it into a `KNNGraph`. This might take some time the first load, as the processed files are built, but subsequent runs will be speedy.\n",
    "\n",
    "TODO: lots of warnings from astropy units when first processing. Gotta see what we can do about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8de251a-d32c-4d72-867c-21ed7595eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = IcaroDataset(os.getcwd(), pre_transform = ttr.KNNGraph(k=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc062311-e60b-4056-964f-0454d689b5f0",
   "metadata": {},
   "source": [
    "We now define a `Net` model, with parametrable number of GCN layers, in channels, hidden channels, and out channels. Each layer but the last has a user-given activation function (`relu` is the default) and a `softmax` output activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d96fee35-7ea9-4aee-8f0c-d2f1f0accb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import MLP, GINConv, global_add_pool\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, activation_function=F.relu):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "            in_channels = hidden_channels\n",
    "\n",
    "        self.last_conv = GCNConv(hidden_channels, out_channels)\n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation_function(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.last_conv(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bbe1d-23bf-4ec3-bc9c-f2f91c501a79",
   "metadata": {},
   "source": [
    "Let us now set up the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53cdb1f1-bac5-401a-8e79-9941582d9fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset    = ds\n",
    "lr         = 0.01\n",
    "device     = torch.device('cpu') # cries in low GPU memory space YoY\n",
    "#device     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 1\n",
    "epochs     = 100\n",
    "hidden_channels = 5\n",
    "num_layers = 2\n",
    "activation_function = torch.sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65a420-1fe6-4258-b5ee-291230458228",
   "metadata": {},
   "source": [
    "Then we'll split and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eb40cdb-5bbc-45da-af42-939a3d6342d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "#print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "\n",
    "#model = Net(dataset.num_node_features*16, hidden_channels, dataset.num_classes, num_layers).to(device)\n",
    "model = Net(dataset.num_node_features, hidden_channels, dataset.num_classes, num_layers, activation_function).to(device)\n",
    "#print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "train_dataset = dataset[len(dataset) // 5:]\n",
    "train_loader  = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "valid_dataset = dataset[len(dataset) // 10:len(dataset) // 5]\n",
    "valid_loader  = DataLoader(valid_dataset, batch_size+1)\n",
    "test_dataset  = dataset[:len(dataset) // 10]\n",
    "test_loader   = DataLoader(test_dataset, batch_size+1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "849284eb-ee04-4f9c-9b57-c6c3b9359588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35698648-7837-4f42-aec6-393b86c49d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def total_len(dataset):\n",
    "    \"\"\"Returns the number of target rows of the dataset\"\"\"\n",
    "    return np.sum([len(data.y) for data in dataset])\n",
    "\n",
    "def total_positives(dataset):\n",
    "    \"\"\"Returns the number of target value '1' of the dataset (only if the other class is '0')\"\"\"\n",
    "    return np.sum([data.y.sum().item() for data in dataset])\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        loader = train_loader\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data).to(device)\n",
    "        pred = out.argmax(dim=-1)\n",
    "        totpos = total_positives(loader.dataset)\n",
    "        totlen = total_len(loader.dataset)\n",
    "        true_positives = torch.logical_and(pred == 1, pred == data.y).sum().int()/totpos\n",
    "        true_negatives = torch.logical_and(pred == 0, pred == data.y).sum().int()/(totlen-totpos)\n",
    "        frac, rev_frac = data.y.sum().item()/len(data.y), (len(data.y) - data.y.sum().item())/len(data.y)\n",
    "        assert not np.isnan(frac) and not np.isnan(rev_frac)\n",
    "        if frac == 0: # in this case placeholder parameters must be enforced to avoid unwanted behavior\n",
    "            frac = rev_frac = 0.5\n",
    "            true_positives = 1.\n",
    "        addloss = (true_positives*true_negatives)**(-0.5) - 1 # scares the model out of giving a constant answer\n",
    "        loss = F.cross_entropy(out, data.y, weight=torch.tensor([frac, rev_frac]).to(device)) + addloss\n",
    "        assert not torch.isnan(loss.detach()), f\"out: {out}\\ndata.y: {data.y}\\nLoss: {total_loss}\\nWeight: {frac}\"\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        #print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "        del data\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect\n",
    "        #print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "    return total_loss / total_len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_correct         = 0\n",
    "    total_true_positives  = 0\n",
    "    total_false_positives = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data).argmax(dim=-1)\n",
    "        total_correct += int((pred == data.y).sum())\n",
    "        total_true_positives += int(np.logical_and(pred == 1, pred == data.y).sum())\n",
    "        total_false_positives += int(np.logical_and(pred == 1, pred != data.y).sum())\n",
    "        del data\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect\n",
    "    totlen = total_len(loader.dataset)\n",
    "    totpos = total_positives(loader.dataset)\n",
    "    return (total_correct/totlen, \n",
    "            total_true_positives/totpos, \n",
    "            total_false_positives/(totlen-totpos)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe9a4a8-c112-4938-998c-6170c320b44c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\tAbsLogLoss: inf\n",
      "\tTrain_accuracy: 0.28024464135437405\n",
      "\tTrain_true_positives: 0.884815275110157\n",
      "\tTrain_false_positives: 0.7205304929896761\n",
      "\tTest_accuracy: 0.21566458747387282\n",
      "\tTest_true_positives: 0.879416282642089\n",
      "\tTest_false_positives: 0.7860145118310573\n",
      "Epoch: 2\n",
      "\tAbsLogLoss: inf\n",
      "\tTrain_accuracy: 0.2808388046655202\n",
      "\tTrain_true_positives: 0.8846458027341544\n",
      "\tTrain_false_positives: 0.7199353506031095\n",
      "\tTest_accuracy: 0.21596091363032488\n",
      "\tTest_true_positives: 0.879416282642089\n",
      "\tTest_false_positives: 0.7857174360553622\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     train_acc, train_tp, train_fp \u001b[38;5;241m=\u001b[39m test(train_loader)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#print(\"validation\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [14], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m pred \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m totpos \u001b[38;5;241m=\u001b[39m total_positives(loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m---> 20\u001b[0m totlen \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_len\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m true_positives \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogical_and(pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, pred \u001b[38;5;241m==\u001b[39m data\u001b[38;5;241m.\u001b[39my)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m/\u001b[39mtotpos\n\u001b[1;32m     22\u001b[0m true_negatives \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogical_and(pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, pred \u001b[38;5;241m==\u001b[39m data\u001b[38;5;241m.\u001b[39my)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m/\u001b[39m(totlen\u001b[38;5;241m-\u001b[39mtotpos)\n",
      "Cell \u001b[0;32mIn [14], line 3\u001b[0m, in \u001b[0;36mtotal_len\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal_len\u001b[39m(dataset):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the number of target rows of the dataset\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum([\u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39my) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset])\n",
      "Cell \u001b[0;32mIn [14], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal_len\u001b[39m(dataset):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the number of target rows of the dataset\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum([\u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39my) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset])\n",
      "File \u001b[0;32m~/miniconda3/envs/SAS_PYTHON/lib/python3.10/site-packages/torch_geometric/data/dataset.py:239\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03mpresent).\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03mIn case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a list, a\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03mtuple, or a :obj:`torch.Tensor` or :obj:`np.ndarray` of type long or\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03mbool, will return a subset of the dataset at the specified indices.\"\"\"\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[0;32m--> 239\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Cell \u001b[0;32mIn [8], line 47\u001b[0m, in \u001b[0;36mIcaroDataset.get\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 47\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mosp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/SAS_PYTHON/lib/python3.10/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/SAS_PYTHON/lib/python3.10/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/SAS_PYTHON/lib/python3.10/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/miniconda3/envs/SAS_PYTHON/lib/python3.10/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m    995\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_UntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[1;32m   1001\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train()\n",
    "    train_acc, train_tp, train_fp = test(train_loader)\n",
    "    #print(\"validation\")\n",
    "    test_acc, test_tp, test_fp = test(valid_loader)\n",
    "    log(Epoch=epoch, \n",
    "        AbsLogLoss=np.log(loss), \n",
    "        Train_accuracy=train_acc,\n",
    "        Train_true_positives=train_tp,\n",
    "        Train_false_positives=train_fp,\n",
    "        Test_accuracy=test_acc,\n",
    "        Test_true_positives=test_tp,\n",
    "        Test_false_positives=test_fp,\n",
    "        logfile=\"logs.log\",\n",
    "        forcemode=\"w\"\n",
    "       )\n",
    "    if not epoch % 10:\n",
    "        sp.run([\"spd-say\", \"'Epoch! Epoch! Epoch!'\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32629856-483f-4949-9587-4a184e1fe8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    sp.run([\"spd-say\", \"'Your process is done'\"])\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205ede8-ea00-4b08-b98c-7e7ba57b374c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAS_PYTHON",
   "language": "python",
   "name": "sas_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
